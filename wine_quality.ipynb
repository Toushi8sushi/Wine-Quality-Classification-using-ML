{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b98be796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72811417",
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wine_file_path=r'T:\\TOSHITH\\PROGRAMMING\\wine+quality\\winequality-red.csv'\n",
    "white_wine_file_path=r'T:\\TOSHITH\\PROGRAMMING\\wine+quality\\winequality-white.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414f2fbb",
   "metadata": {},
   "source": [
    "### step 1 combine data and preprocess it for red and white wine seperately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66928a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1518, 12) (4535, 12)\n",
      "quality\n",
      "5    681\n",
      "6    638\n",
      "7    199\n",
      "Name: count, dtype: int64\n",
      "quality\n",
      "6    2198\n",
      "5    1457\n",
      "7     880\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "red_wine_df=pd.read_csv(red_wine_file_path,sep=';')\n",
    "white_wine_df=pd.read_csv(white_wine_file_path,sep=';')\n",
    "\n",
    "unwanted_qualities = [ 3, 9,8,4]\n",
    "\n",
    "# Filter out those rows from red and white wine data\n",
    "red_wine_df = red_wine_df[~red_wine_df['quality'].isin(unwanted_qualities)]\n",
    "white_wine_df = white_wine_df[~white_wine_df['quality'].isin(unwanted_qualities)]\n",
    "\n",
    "print(red_wine_df.shape,white_wine_df.shape)\n",
    "#red_wine_df.head()\n",
    "print(red_wine_df['quality'].value_counts())\n",
    "print(white_wine_df['quality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9e3f7",
   "metadata": {},
   "source": [
    "So we see that the data is unbalanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fde6ae56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is this really necessairy as some row values have become negative'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#function to scale our data to make it easier for our model to learn \n",
    "\n",
    "def scaler(wine_df):\n",
    "    columns=list(wine_df.columns)\n",
    "\n",
    "    wine_df=StandardScaler().fit_transform(wine_df)\n",
    "    wine_df=pd.DataFrame(wine_df,columns=columns)\n",
    "    return wine_df\n",
    "\n",
    "'''is this really necessairy as some row values have become negative'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2566c0",
   "metadata": {},
   "source": [
    "### 1. Undersampling\n",
    "* As white is more we will reduce it's numbers to equal reds\n",
    "* Also we will do a train test split before sampling as we dont want to introduce redundant samples in the training and testing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d05b4901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1062, 12), (456, 12), (4079, 12), (456, 12))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#70:30 split \n",
    "split_ratio=0.3\n",
    "#the data frame in train and test still contains the quality column so we have garbage values which we will not use\n",
    "\n",
    "red_wine_train, red_wine_test, garbage1,garbage2 = train_test_split(red_wine_df,red_wine_df['quality'],test_size=split_ratio)\n",
    "\n",
    "\n",
    "#we want same no of samples in white wine test as red wine \n",
    "\n",
    "white_wine_test=white_wine_df.sample(n=red_wine_test.shape[0])\n",
    "white_wine_train=white_wine_df.drop(white_wine_test.index)\n",
    "\n",
    "red_wine_train.shape,red_wine_test.shape,white_wine_train.shape, white_wine_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f4f57",
   "metadata": {},
   "source": [
    "encoding red as 0 and white as 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ae9d1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_train_and_test(red_wine_tr,white_wine_tr,red_wine_test,white_wine_test):\n",
    "    # Add 'colour' column\n",
    "    red_wine_tr['colour'] = 0\n",
    "    white_wine_tr['colour'] = 1\n",
    "    red_wine_test['colour'] = 0\n",
    "    white_wine_test['colour'] = 1\n",
    "\n",
    "    # Drop 'quality' for X, but keep it for Y\n",
    "    X_train = pd.concat([\n",
    "        red_wine_tr.drop(columns=['quality']),\n",
    "        white_wine_tr.drop(columns=['quality'])\n",
    "    ], axis=0)\n",
    "\n",
    "    Y_train = pd.concat([\n",
    "        red_wine_tr['quality'],\n",
    "        white_wine_tr['quality']\n",
    "    ], axis=0)\n",
    "\n",
    "    X_test = pd.concat([\n",
    "        red_wine_test.drop(columns=['quality']),\n",
    "        white_wine_test.drop(columns=['quality'])\n",
    "    ], axis=0)\n",
    "\n",
    "    Y_test = pd.concat([\n",
    "        red_wine_test['quality'],\n",
    "        white_wine_test['quality']\n",
    "    ], axis=0)\n",
    "\n",
    "    return X_train,X_test,Y_train,Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49d4ef",
   "metadata": {},
   "source": [
    "US is for under sample , N is for normal with out sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "57e8505e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2124, 12), (912, 12), (2124,), (912,), (5141, 12))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to undersample white \n",
    "no_of_red=red_wine_train.shape[0] #shape of train y\n",
    "\n",
    "N_X_train,N_X_test,N_Y_train,N_Y_test=return_train_and_test(red_wine_train,white_wine_train,red_wine_test,white_wine_test)\n",
    "\n",
    "sample_white= white_wine_train.sample(no_of_red) # we get equal no of columns \n",
    "US_X_train,US_X_test,US_Y_train,US_Y_test=return_train_and_test(red_wine_train,sample_white,red_wine_test,white_wine_test)\n",
    "\n",
    "US_X_train.shape,US_X_test.shape,US_Y_train.shape,US_Y_test.shape,N_X_train.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b1444365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>colour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.671752</td>\n",
       "      <td>0.777839</td>\n",
       "      <td>0.413276</td>\n",
       "      <td>2.608559</td>\n",
       "      <td>3.366086</td>\n",
       "      <td>1.717095</td>\n",
       "      <td>0.02764</td>\n",
       "      <td>2.915765</td>\n",
       "      <td>-0.402582</td>\n",
       "      <td>1.172799</td>\n",
       "      <td>-1.256578</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0       1.671752          0.777839     0.413276        2.608559   3.366086   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide   density        pH  sulphates  \\\n",
       "0             1.717095               0.02764  2.915765 -0.402582   1.172799   \n",
       "\n",
       "    alcohol  colour  \n",
       "0 -1.256578     0.0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US_X_train=scaler(US_X_train)\n",
    "US_X_test=scaler(US_X_test)\n",
    "#as the colour column has changed because of the standard scaler \n",
    "US_X_train['colour'] = US_X_train['colour'].replace(-1, 0)\n",
    "US_X_test['colour'] = US_X_test['colour'].replace(-1, 0)\n",
    "\n",
    "US_X_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1c658dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>colour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5136</th>\n",
       "      <td>-0.533573</td>\n",
       "      <td>-0.629220</td>\n",
       "      <td>0.412269</td>\n",
       "      <td>-0.898051</td>\n",
       "      <td>-0.660118</td>\n",
       "      <td>-0.152410</td>\n",
       "      <td>-0.151086</td>\n",
       "      <td>-0.555205</td>\n",
       "      <td>0.489557</td>\n",
       "      <td>0.104559</td>\n",
       "      <td>-0.644794</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5137</th>\n",
       "      <td>-0.452325</td>\n",
       "      <td>-0.033458</td>\n",
       "      <td>0.268940</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>-0.223150</td>\n",
       "      <td>1.486856</td>\n",
       "      <td>0.872255</td>\n",
       "      <td>0.080419</td>\n",
       "      <td>-0.394488</td>\n",
       "      <td>-0.453632</td>\n",
       "      <td>-0.729081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5138</th>\n",
       "      <td>-0.533573</td>\n",
       "      <td>-0.563024</td>\n",
       "      <td>-0.949355</td>\n",
       "      <td>-0.918503</td>\n",
       "      <td>-0.397938</td>\n",
       "      <td>-0.093865</td>\n",
       "      <td>-0.169360</td>\n",
       "      <td>-0.700869</td>\n",
       "      <td>-1.404825</td>\n",
       "      <td>-0.453632</td>\n",
       "      <td>-0.897655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5139</th>\n",
       "      <td>-1.346053</td>\n",
       "      <td>-0.232046</td>\n",
       "      <td>-0.161046</td>\n",
       "      <td>-0.938955</td>\n",
       "      <td>-0.951430</td>\n",
       "      <td>-0.679317</td>\n",
       "      <td>-0.187634</td>\n",
       "      <td>-1.975428</td>\n",
       "      <td>0.805287</td>\n",
       "      <td>-1.011824</td>\n",
       "      <td>1.968093</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5140</th>\n",
       "      <td>-0.939813</td>\n",
       "      <td>-0.761612</td>\n",
       "      <td>0.412269</td>\n",
       "      <td>-1.000311</td>\n",
       "      <td>-1.009693</td>\n",
       "      <td>-0.562226</td>\n",
       "      <td>-0.406921</td>\n",
       "      <td>-1.737069</td>\n",
       "      <td>0.300119</td>\n",
       "      <td>-1.430468</td>\n",
       "      <td>1.125226</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "5136      -0.533573         -0.629220     0.412269       -0.898051  -0.660118   \n",
       "5137      -0.452325         -0.033458     0.268940        0.472229  -0.223150   \n",
       "5138      -0.533573         -0.563024    -0.949355       -0.918503  -0.397938   \n",
       "5139      -1.346053         -0.232046    -0.161046       -0.938955  -0.951430   \n",
       "5140      -0.939813         -0.761612     0.412269       -1.000311  -1.009693   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide   density        pH  \\\n",
       "5136            -0.152410             -0.151086 -0.555205  0.489557   \n",
       "5137             1.486856              0.872255  0.080419 -0.394488   \n",
       "5138            -0.093865             -0.169360 -0.700869 -1.404825   \n",
       "5139            -0.679317             -0.187634 -1.975428  0.805287   \n",
       "5140            -0.562226             -0.406921 -1.737069  0.300119   \n",
       "\n",
       "      sulphates   alcohol  colour  \n",
       "5136   0.104559 -0.644794       1  \n",
       "5137  -0.453632 -0.729081       1  \n",
       "5138  -0.453632 -0.897655       1  \n",
       "5139  -1.011824  1.968093       1  \n",
       "5140  -1.430468  1.125226       1  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_X_train = scaler(N_X_train)\n",
    "N_X_test = scaler(N_X_test)\n",
    "# as these 2 are unbalenced data ill change the colour column to 0 and 1 manually \n",
    "#can apply this to above as well \n",
    "\n",
    "unique_values = N_X_train['colour'].unique()\n",
    "lower_value = min(unique_values)\n",
    "\n",
    "N_X_train['colour'] = (N_X_train['colour'] != lower_value).astype(int)\n",
    "\n",
    "unique_values = N_X_test['colour'].unique()\n",
    "lower_value = min(unique_values)\n",
    "N_X_test['colour'] = (N_X_test['colour'] != lower_value).astype(int)\n",
    "\n",
    "N_X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform train test split before sampling and sampling should be done only on training data \n",
    "\n",
    "#also verify if the data column has the same no of 1 and 0 (y values) values 11:57 in the video \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5554f",
   "metadata": {},
   "source": [
    "2 upsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9333784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colour\n",
      "1    4079\n",
      "0    1062\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(N_X_train['colour'].value_counts()) # w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "daf5a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to avoid index misalignment\n",
    "N_X_train = N_X_train.reset_index(drop=True)\n",
    "N_Y_train = N_Y_train.reset_index(drop=True)\n",
    "\n",
    "# Split by colour\n",
    "red_mask = N_X_train['colour'] == 0\n",
    "red_X = N_X_train[red_mask]\n",
    "red_Y = N_Y_train[red_mask]\n",
    "\n",
    "white_mask = N_X_train['colour'] == 1\n",
    "white_X = N_X_train[white_mask]\n",
    "white_Y = N_Y_train[white_mask]\n",
    "\n",
    "# Now you can upsample\n",
    "from sklearn.utils import resample\n",
    "red_X_upsampled, red_Y_upsampled = resample(\n",
    "    red_X, red_Y,\n",
    "    replace=True,\n",
    "    n_samples=len(white_X),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "UP_X_train = pd.concat([red_X_upsampled, white_X])\n",
    "UP_Y_train = pd.concat([red_Y_upsampled, white_Y])\n",
    "UP_X_test = US_X_test.copy()\n",
    "UP_Y_test = US_Y_test.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "03106d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colour\n",
      "0    4079\n",
      "1    4079\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(UP_X_train['colour'].value_counts()) # w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825ba2e",
   "metadata": {},
   "source": [
    "### apply smote "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d96ad858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "183f889e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: Counter({6: 2427, 5: 1791, 7: 923})\n"
     ]
    }
   ],
   "source": [
    "print(\"Original class distribution:\", Counter(N_Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "15670da3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The {8, 4} target class is/are not present in the data.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m sampling_strategy = {\u001b[32m4\u001b[39m: \u001b[32m500\u001b[39m, \u001b[32m8\u001b[39m: \u001b[32m500\u001b[39m}\n\u001b[32m      3\u001b[39m smote = SMOTE(sampling_strategy=sampling_strategy, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m SM_X_train, SM_Y_train = \u001b[43msmote\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_Y_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mResampled class distribution:\u001b[39m\u001b[33m\"\u001b[39m, Counter(SM_Y_train))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\base.py:202\u001b[39m, in \u001b[36mBaseSampler.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, **params):\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \u001b[33;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\base.py:101\u001b[39m, in \u001b[36mSamplerMixin.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m     98\u001b[39m arrays_transformer = ArraysTransformer(X, y)\n\u001b[32m     99\u001b[39m X, y, binarize_y = \u001b[38;5;28mself\u001b[39m._check_X_y(X, y)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28mself\u001b[39m.sampling_strategy_ = \u001b[43mcheck_sampling_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampling_type\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m output = \u001b[38;5;28mself\u001b[39m._fit_resample(X, y, **params)\n\u001b[32m    107\u001b[39m y_ = (\n\u001b[32m    108\u001b[39m     label_binarize(output[\u001b[32m1\u001b[39m], classes=np.unique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[32m1\u001b[39m]\n\u001b[32m    109\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\utils\\_validation.py:557\u001b[39m, in \u001b[36mcheck_sampling_strategy\u001b[39m\u001b[34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[32m    553\u001b[39m         \u001b[38;5;28msorted\u001b[39m(SAMPLING_TARGET_KIND[sampling_strategy](y, sampling_type).items())\n\u001b[32m    554\u001b[39m     )\n\u001b[32m    555\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampling_strategy, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    556\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m         \u001b[38;5;28msorted\u001b[39m(\u001b[43m_sampling_strategy_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m)\u001b[49m.items())\n\u001b[32m    558\u001b[39m     )\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampling_strategy, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[32m    561\u001b[39m         \u001b[38;5;28msorted\u001b[39m(_sampling_strategy_list(sampling_strategy, y, sampling_type).items())\n\u001b[32m    562\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\imblearn\\utils\\_validation.py:319\u001b[39m, in \u001b[36m_sampling_strategy_dict\u001b[39m\u001b[34m(sampling_strategy, y, sampling_type)\u001b[39m\n\u001b[32m    315\u001b[39m set_diff_sampling_strategy_target = \u001b[38;5;28mset\u001b[39m(sampling_strategy.keys()) - \u001b[38;5;28mset\u001b[39m(\n\u001b[32m    316\u001b[39m     target_stats.keys()\n\u001b[32m    317\u001b[39m )\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(set_diff_sampling_strategy_target) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mset_diff_sampling_strategy_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m target class is/are not \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpresent in the data.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m     )\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# check that there is no negative number\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(n_samples < \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n_samples \u001b[38;5;129;01min\u001b[39;00m sampling_strategy.values()):\n",
      "\u001b[31mValueError\u001b[39m: The {8, 4} target class is/are not present in the data."
     ]
    }
   ],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "sampling_strategy = {4: 500, 8: 500}\n",
    "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=42)\n",
    "SM_X_train, SM_Y_train = smote.fit_resample(N_X_train, N_Y_train)\n",
    "print(\"Resampled class distribution:\", Counter(SM_Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8de23c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colour\n",
      "1    4946\n",
      "0    1204\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(SM_X_train['colour'].value_counts()) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586013f5",
   "metadata": {},
   "source": [
    "### apply ensamble technique on your data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2febbf",
   "metadata": {},
   "source": [
    "### use different ML model for the same data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21fee7b",
   "metadata": {},
   "source": [
    "XGBoost classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "77f16fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best Hyperparameters: {'subsample': 0.6, 'reg_lambda': 1, 'reg_alpha': 0, 'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.01, 'gamma': 0, 'colsample_bytree': 1.0}\n"
     ]
    }
   ],
   "source": [
    "'''from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Get unique labels and encode them\n",
    "unique_labels = sorted(set(US_Y_train) | set(US_Y_test))\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "US_Y_train_encoded = le.transform(US_Y_train)\n",
    "\n",
    "model = XGBClassifier(objective='multi:softmax', num_class=len(unique_labels), random_state=42)\n",
    "model.fit(US_X_train, US_Y_train_encoded)\n",
    "\n",
    "y_pred_encoded = model.predict(US_X_test)\n",
    "US_y_pred = le.inverse_transform(y_pred_encoded)'''\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Encode labels\n",
    "unique_labels = sorted(set(US_Y_train) | set(US_Y_test))\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "US_Y_train_encoded = le.transform(US_Y_train)\n",
    "US_Y_test_encoded = le.transform(US_Y_test)  # optional, in case needed later\n",
    "\n",
    "# Define base model\n",
    "xgb = XGBClassifier(objective='multi:softmax', num_class=len(unique_labels), random_state=42)\n",
    "\n",
    "# Define hyperparameter grid for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 1, 2, 5],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "    'reg_lambda': [0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "random_search.fit(US_X_train, US_Y_train_encoded)\n",
    "\n",
    "# Get best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Predict and decode labels\n",
    "y_pred_encoded = best_model.predict(US_X_test)\n",
    "US_y_pred = le.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c79502d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Get unique labels and encode them\n",
    "unique_labels = sorted(set(N_Y_train) | set(N_Y_test))\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "N_Y_train_encoded = le.transform(N_Y_train)\n",
    "N_Y_test_encoded = le.transform(N_Y_test)\n",
    "\n",
    "model = XGBClassifier(objective='multi:softmax', num_class=len(unique_labels), random_state=42)\n",
    "model.fit(N_X_train, N_Y_train_encoded)\n",
    "\n",
    "y_pred_encoded = model.predict(N_X_test)\n",
    "N_y_pred = le.inverse_transform(y_pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8ff3ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Get unique labels and encode them\n",
    "unique_labels = sorted(set(UP_Y_train) | set(UP_Y_test))\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "UP_Y_train_encoded = le.transform(UP_Y_train)\n",
    "UP_Y_test_encoded = le.transform(UP_Y_test)\n",
    "\n",
    "model = XGBClassifier(objective='multi:softmax', num_class=len(unique_labels), random_state=42)\n",
    "model.fit(UP_X_train, UP_Y_train_encoded)\n",
    "\n",
    "y_pred_encoded = model.predict(UP_X_test)\n",
    "UP_y_pred = le.inverse_transform(y_pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "984f001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Get unique labels and encode them\n",
    "unique_labels = sorted(set(SM_Y_train) | set(N_Y_test))\n",
    "le = LabelEncoder()\n",
    "le.fit(unique_labels)\n",
    "\n",
    "SM_Y_train_encoded = le.transform(SM_Y_train)\n",
    "\n",
    "model = XGBClassifier(objective='multi:softmax', num_class=len(unique_labels), random_state=42)\n",
    "model.fit(SM_X_train, SM_Y_train_encoded)\n",
    "\n",
    "y_pred_encoded = model.predict(N_X_test)\n",
    "SM_y_pred = le.inverse_transform(y_pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d8a6181f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Undersampled data: 0.6348684210526315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           5       0.70      0.71      0.70       347\n",
      "           6       0.60      0.66      0.63       409\n",
      "           7       0.59      0.40      0.48       156\n",
      "\n",
      "    accuracy                           0.63       912\n",
      "   macro avg       0.63      0.59      0.60       912\n",
      "weighted avg       0.63      0.63      0.63       912\n",
      "\n",
      "Accuracy for normal data: 0.5712719298245614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           5       0.65      0.61      0.63       347\n",
      "           6       0.53      0.67      0.59       409\n",
      "           7       0.54      0.22      0.32       156\n",
      "\n",
      "    accuracy                           0.57       912\n",
      "   macro avg       0.57      0.50      0.51       912\n",
      "weighted avg       0.57      0.57      0.56       912\n",
      "\n",
      "Accuracy for normal data: 0.5679824561403509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           5       0.64      0.57      0.60       347\n",
      "           6       0.53      0.68      0.60       409\n",
      "           7       0.54      0.28      0.37       156\n",
      "\n",
      "    accuracy                           0.57       912\n",
      "   macro avg       0.57      0.51      0.52       912\n",
      "weighted avg       0.57      0.57      0.56       912\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"Accuracy for normal data:\", accuracy_score(N_Y_test, SM_y_pred))\\nprint(classification_report(N_Y_test, SM_y_pred))'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Accuracy for Undersampled data:\", accuracy_score(US_Y_test, US_y_pred))\n",
    "print(classification_report(US_Y_test, US_y_pred))\n",
    "print(\"Accuracy for normal data:\", accuracy_score(N_Y_test, N_y_pred))\n",
    "print(classification_report(N_Y_test, N_y_pred))\n",
    "print(\"Accuracy for normal data:\", accuracy_score(UP_Y_test, UP_y_pred))\n",
    "print(classification_report(UP_Y_test, UP_y_pred))\n",
    "'''\n",
    "print(\"Accuracy for normal data:\", accuracy_score(N_Y_test, SM_y_pred))\n",
    "print(classification_report(N_Y_test, SM_y_pred))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c07a1ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 Cross-Validation on TRAINING Set:\n",
      "Scores: [0.64941176 0.62588235 0.65176471 0.59529412 0.61084906]\n",
      "Mean Accuracy: 0.6266\n",
      "Standard Deviation: 0.0218\n",
      "\n",
      "🧪 Evaluation on TEST Set:\n",
      "Accuracy: 0.14912280701754385\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.21      0.28      0.24       347\n",
      "           6       0.37      0.10      0.15       409\n",
      "           7       0.00      0.00      0.00       156\n",
      "\n",
      "    accuracy                           0.15       912\n",
      "   macro avg       0.15      0.09      0.10       912\n",
      "weighted avg       0.25      0.15      0.16       912\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\toshi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1️⃣ Cross-validation on TRAINING data\n",
    "train_cv_scores = cross_val_score(\n",
    "    best_model,\n",
    "    US_X_train,\n",
    "    US_Y_train_encoded,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n💡 Cross-Validation on TRAINING Set:\")\n",
    "print(f\"Scores: {train_cv_scores}\")\n",
    "print(f\"Mean Accuracy: {np.mean(train_cv_scores):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(train_cv_scores):.4f}\")\n",
    "\n",
    "# 2️⃣ Evaluation on TEST data\n",
    "y_test_pred_encoded = best_model.predict(US_X_test)\n",
    "y_test_pred = le.inverse_transform(y_test_pred_encoded)\n",
    "\n",
    "print(\"\\n🧪 Evaluation on TEST Set:\")\n",
    "print(\"Accuracy:\", accuracy_score(US_Y_test, y_test_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(US_Y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68f91c",
   "metadata": {},
   "source": [
    "XGBoost Regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8b921e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "model = XGBRegressor(random_state=42)\n",
    "model.fit(US_X_train, US_Y_train)\n",
    "\n",
    "y_pred = model.predict(US_X_test)\n",
    "US_y_pred_rounded = np.clip(np.round(y_pred), min(US_Y_train), max(US_Y_train)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1d2a57a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(random_state=42)\n",
    "model.fit(N_X_train, N_Y_train)\n",
    "\n",
    "y_pred = model.predict(N_X_test)\n",
    "N_y_pred_rounded = np.clip(np.round(y_pred), min(N_Y_train), max(N_Y_train)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9619be1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6162280701754386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           5       0.69      0.64      0.66       347\n",
      "           6       0.57      0.69      0.63       409\n",
      "           7       0.60      0.36      0.45       156\n",
      "\n",
      "    accuracy                           0.62       912\n",
      "   macro avg       0.62      0.56      0.58       912\n",
      "weighted avg       0.62      0.62      0.61       912\n",
      "\n",
      "Accuracy: 0.5712719298245614\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           5       0.67      0.55      0.60       347\n",
      "           6       0.53      0.70      0.61       409\n",
      "           7       0.49      0.28      0.35       156\n",
      "\n",
      "    accuracy                           0.57       912\n",
      "   macro avg       0.57      0.51      0.52       912\n",
      "weighted avg       0.58      0.57      0.56       912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(US_Y_test, US_y_pred_rounded))\n",
    "print(classification_report(US_Y_test, US_y_pred_rounded))\n",
    "print(\"Accuracy:\", accuracy_score(N_Y_test, N_y_pred_rounded))\n",
    "print(classification_report(N_Y_test, N_y_pred_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146f806",
   "metadata": {},
   "source": [
    "### Random Forest regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d1391c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "80c281c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "rf_regressor.fit(US_X_train, US_Y_train)\n",
    "US_y_pred = rf_regressor.predict(US_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fe31e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regressor = RandomForestRegressor(random_state=42)\n",
    "rf_regressor.fit(N_X_train, N_Y_train)\n",
    "N_y_pred = rf_regressor.predict(N_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ae5a585a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error Under sampled: 0.2977084429824562\n",
      "R² Score Under sampled: 0.4135837084226355\n",
      "Mean Squared Error Normal: 0.353253399122807\n",
      "R² Score Normal: 0.3041730821423073\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Squared Error Under sampled:\", mean_squared_error(US_Y_test, US_y_pred))\n",
    "print(\"R² Score Under sampled:\", r2_score(US_Y_test, US_y_pred))\n",
    "print(\"Mean Squared Error Normal:\", mean_squared_error(N_Y_test, N_y_pred))\n",
    "print(\"R² Score Normal:\", r2_score(N_Y_test, N_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70379dc",
   "metadata": {},
   "source": [
    "### random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c325282",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6f2d848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "US_Y_train_encoded = le.fit_transform(US_Y_train)\n",
    "US_Y_test_encoded = le.transform(US_Y_test)\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(US_X_train, US_Y_train_encoded)\n",
    "y_pred_encoded = rf_classifier.predict(US_X_test)\n",
    "US_y_pred = le.inverse_transform(y_pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9b490632",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_Y_train_encoded = le.fit_transform(N_Y_train)\n",
    "N_Y_test_encoded = le.transform(N_Y_test)\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(N_X_train, N_Y_train_encoded)\n",
    "y_pred_encoded = rf_classifier.predict(N_X_test)\n",
    "N_y_pred = le.inverse_transform(y_pred_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e7839d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Undersampled data: 0.6600877192982456\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           5       0.72      0.71      0.71       347\n",
      "           6       0.62      0.70      0.66       409\n",
      "           7       0.65      0.45      0.53       156\n",
      "\n",
      "    accuracy                           0.66       912\n",
      "   macro avg       0.66      0.62      0.63       912\n",
      "weighted avg       0.66      0.66      0.66       912\n",
      "\n",
      "Accuracy for normal data: 0.5745614035087719\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           5       0.67      0.57      0.62       347\n",
      "           6       0.53      0.71      0.61       409\n",
      "           7       0.53      0.22      0.31       156\n",
      "\n",
      "    accuracy                           0.57       912\n",
      "   macro avg       0.58      0.50      0.51       912\n",
      "weighted avg       0.58      0.57      0.56       912\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for Undersampled data:\", accuracy_score(US_Y_test, US_y_pred))\n",
    "print(classification_report(US_Y_test, US_y_pred))\n",
    "print(\"Accuracy for normal data:\", accuracy_score(N_Y_test, N_y_pred))\n",
    "print(classification_report(N_Y_test, N_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a625b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
